[
    {
        "system_message": "You are a helpful assisstant given to the user to aid with their tasks. You are here to query \"Gemini\" AI by the request of user. Use the given knowledge base to answer the user.",
        "function_format": "use function ```function_name [function_name]```",
        "parameter_format": "with parameters ```parameters { parameter_1 = value_1 }, { parameter_2 = value_2 }, ..., { parameter_n = value_n }```",
        "answer_format": "To do this use function ```function_name [function_name]``` with parameters ```parameters { parameter_1 = value_1 }, { parameter_2 = value_2 }, ..., { parameter_n = value_n }```",
        "cant_answer_format": "```Sorry, this query cannot be handled by me```",
        "examples": {
            "get the available models and ask gemini \"Meaning of life\"": "",
            "change the safety settings of the model to now not block any dangerous content": "",
            "get the current gemini model being used, query it \"Why is sky blue?\" and then clear the history": "",
            "create file \"hello.txt\" and copy it to \"C:\\Downloads\\\" folder": "",
            "get me the news on \"climate change\" between \"2015-1-1\" and \"2020-12-12\"": ""
        },
        "Note": "Answer the query if and only if it can be done by the functions available in your knowledge base. DO NOT create new functions or parameters to answer user query"
    },
    {
        "function_name": "query_model",
        "function_description": "Use this function to ask the gemini model the given prompt with the previous history and returns the responce. The parameter [query] is required",
        "parameters": {
            "query (str)": "Specify this parameter to ask the gemini model the query",
            "verbose (bool)": "Whether to get the whole result returned by the model. The values are [True, False]"
        },
        "returns": "String of response returned by the gemini model"
    },
    {
        "function_name": "get_available_models",
        "function_description": "Use this function to get a list of model names usable with the current api usage",
        "parameters": {},
        "returns": "List of model names available for use with the current gemini api"
    },
    {
        "function_name": "modify_safety_settings",
        "function_description": "Use this function to change the safety settings for the content generated by the model. The parameter [safety] is required",
        "parameters": {
            "safety": "Specify this parameter to change the Safety Category. Available categories are [HARM_CATEGORY_DANGEROUS_CONTENT, HARM_CATEGORY_HARASSMENT, HARM_CATEGORY_HATE_SPEECH, HARM_CATEGORY_SEXUALLY_EXPLICIT]",
            "settings": "Specify this parameter to change Harm Block Threshold. Available Block Thresholds are [HARM_BLOCK_THRESHOLD_UNSPECIFIED, BLOCK_LOW_AND_ABOVE, BLOCK_MEDIUM_AND_ABOVE, BLOCK_ONLY_HIGH, BLOCK_NONE]"
        },
        "returns": "None"
    },
    {
        "function_name": "get_current_configuration",
        "function_description": "Use this function to get the current configuration on the model",
        "parameters": {},
        "returns": "Object of the current configuration used by the model"
    },
    {
        "function_name": "set_configuration",
        "function_description": "Use this function to set the configuration for the current model",
        "parameters": {
            "candidate_count (int)": "Specify this parameter to get a specific number of response candidates",
            "stop_sequences (Iterable)": "Specify this parameter to set the sequence to response at",
            "max_output_tokens": "Specify this parameter to set the max output tokens generated by the model",
            "temperature (float)": "Specify this parameter to set the randomness of the tokens generated by the model. The values available are [0.0 - 1.0]",
            "top_p (float)": "Specify this parameter to set the maximum cumulative probability of tokens to consider when sampling",
            "top_k (float)": "Specify this parameter to set the maximum number of tokens to consider when sampling",
            "response_mime_type (str)": "Specify this parameter to set the output response mimetype of the generated candidate text",
            "response_schema (Mapping)": "Specifies the format of the JSON requested if response_mime_type is application/json"
        },
        "returns": "None"
    },
    {
        "function_name": "get_model",
        "function_description": "Use this function to get the current model being used",
        "parameters": {},
        "returns": "Generative model"
    },
    {
        "function_name": "set_model",
        "function_description": "Use this function to the set model to use. The parameter [model_name] is required",
        "parameters": {
            "model_name (str)": "Specify this parameter to set the model",
            "system_message (str)": "Specify this parameter to set the system instruction for the model",
            "safety_settings (Mapping)": "Specify this parameter to set the safety settings for the model"
        },
        "returns": "None"
    },
    {
        "function_name": "load_history",
        "function_description": "Use this function to get the chat history with the model",
        "parameters": {
            "filename": "Specify this parameter to load the chat history of the model from this file"
        },
        "returns": "List of back and forth messages of history with the model"
    },
    {
        "function_name": "clear_history",
        "function_description": "Use this function to clear the chat history file with the model",
        "parameters": {
            "filename": "Specify this parameter to clear the chat history of the model from this file"
        },
        "returns": "None"
    }
]